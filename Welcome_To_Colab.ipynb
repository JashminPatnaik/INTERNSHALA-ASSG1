{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JashminPatnaik/INTERNSHALA-ASSG1/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "# Install necessary packages\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n",
        "\n",
        "# Set up Chrome options\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import csv\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "service = Service('/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "def scrape_olx_car_covers():\n",
        "    url = \"https://www.olx.in/items/q-car-cover\"\n",
        "    driver.get(url)\n",
        "    time.sleep(5)\n",
        "\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(3)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    listings = driver.find_elements(By.XPATH, '//li[@data-aut-id=\"itemBox\"]')\n",
        "\n",
        "    data = []\n",
        "    for listing in listings:\n",
        "        try:\n",
        "            title = listing.find_element(By.XPATH, './/span[@data-aut-id=\"itemTitle\"]').text\n",
        "        except:\n",
        "            title = ''\n",
        "        try:\n",
        "            price = listing.find_element(By.XPATH, './/span[@data-aut-id=\"itemPrice\"]').text\n",
        "        except:\n",
        "            price = ''\n",
        "        try:\n",
        "            location = listing.find_element(By.XPATH, './/span[@data-aut-id=\"item-location\"]').text\n",
        "        except:\n",
        "            location = ''\n",
        "        try:\n",
        "            link = listing.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "        except:\n",
        "            link = ''\n",
        "        data.append([title, price, location, link])\n",
        "\n",
        "    with open('car_covers.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Title', 'Price', 'Location', 'Link'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Scraped {len(data)} listings. Data saved to car_covers.csv\")\n",
        "\n",
        "scrape_olx_car_covers()\n",
        "driver.quit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B8yO-AqYggOk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Welcome to Colab!\n",
        "\n",
        "## Explore the Gemini API\n",
        "The Gemini API gives you access to Gemini models created by Google DeepMind. Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, code, and audio.\n",
        "\n",
        "**How to get started?**\n",
        "*  Go to [Google AI Studio](https://aistudio.google.com/) and log in with your Google account.\n",
        "*  [Create an API key](https://aistudio.google.com/app/apikey).\n",
        "* Use a quickstart for [Python](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb), or call the REST API using [curl](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb).\n",
        "\n",
        "**Discover Gemini's advanced capabilities**\n",
        "*  Play with Gemini [multimodal outputs](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image-out.ipynb), mixing text and images in an iterative way.\n",
        "*  Discover the [multimodal Live API](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb ) (demo [here](https://aistudio.google.com/live)).\n",
        "*  Learn how to [analyze images and detect items in your pictures](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\") using Gemini (bonus, there's a [3D version](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb) as well!).\n",
        "*  Unlock the power of [Gemini thinking model](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb), capable of solving complex task with its inner thoughts.\n",
        "      \n",
        "**Explore complex use cases**\n",
        "*  Use [Gemini grounding capabilities](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_grounding_for_research_report.ipynb) to create a report on a company based on what the model can find on internet.\n",
        "*  Extract [invoices and form data from PDF](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb) in a structured way.\n",
        "*  Create [illustrations based on a whole book](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb) using Gemini large context window and Imagen.\n",
        "\n",
        "To learn more, check out the [Gemini cookbook](https://github.com/google-gemini/cookbook) or visit the [Gemini API documentation](https://ai.google.dev/docs/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n",
        "\n",
        "Variables that you define in one cell can later be used in other cells:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "\"\"\"\n",
        "OLX Car Cover Scraper\n",
        "=====================\n",
        "\n",
        "This script scrapes \"Car Cover\" listings from OLX India and saves the results into a CSV file.\n",
        "It uses the requests and BeautifulSoup libraries for web scraping.\n",
        "\n",
        "Author: Your Name\n",
        "Date: 2025-05-24\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Define the OLX search URL\n",
        "URL = 'https://www.olx.in/items/q-car-cover'\n",
        "\n",
        "# Set User-Agent to mimic a browser visit\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "}\n",
        "\n",
        "# Make a request to OLX\n",
        "response = requests.get(URL, headers=headers)\n",
        "\n",
        "# Parse the response using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Try finding listings (This class may change if OLX updates its site)\n",
        "items = soup.find_all('li', class_='EIR5N')\n",
        "\n",
        "print(f\"âœ… Found {len(items)} listings\")\n",
        "\n",
        "# Prepare the output file\n",
        "output_file = 'olx_car_covers.csv'\n",
        "\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Price', 'Location', 'Link'])  # CSV header\n",
        "\n",
        "    for item in items:\n",
        "        try:\n",
        "            title_tag = item.find('span', {'data-aut-id': 'itemTitle'})\n",
        "            price_tag = item.find('span', {'data-aut-id': 'itemPrice'})\n",
        "            location_tag = item.find('span', {'data-aut-id': 'item-location'})\n",
        "            link_tag = item.find('a', href=True)\n",
        "\n",
        "            if title_tag and price_tag and location_tag and link_tag:\n",
        "                title = title_tag.text.strip()\n",
        "                price = price_tag.text.strip()\n",
        "                location = location_tag.text.strip()\n",
        "                link = 'https://www.olx.in' + link_tag['href']\n",
        "                writer.writerow([title, price, location, link])\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Skipped one item due to error: {e}\")\n",
        "\n",
        "print(f\"ðŸŽ‰ Scraping complete! Data saved to: {os.path.abspath(output_file)}\")\n",
        "from google.colab import files\n",
        "files.download(\"olx_car_covers.csv\")\n"
      ],
      "metadata": {
        "id": "usjixFobfTF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "# Install necessary packages and ChromeDriver\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n",
        "\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from google.colab import files\n",
        "\n",
        "# Configure Chrome options for headless browsing in Colab\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "service = Service('/usr/lib/chromium-browser/chromedriver')\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "def scrape_olx_car_covers():\n",
        "    url = \"https://www.olx.in/items/q-car-cover\"\n",
        "    driver.get(url)\n",
        "    time.sleep(5)\n",
        "\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(3)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    listings = driver.find_elements(By.XPATH, '//li[@data-aut-id=\"itemBox\"]')\n",
        "\n",
        "    data = []\n",
        "    for listing in listings:\n",
        "        try:\n",
        "            title = listing.find_element(By.XPATH, './/span[@data-aut-id=\"itemTitle\"]').text\n",
        "        except:\n",
        "            title = ''\n",
        "        try:\n",
        "            price = listing.find_element(By.XPATH, './/span[@data-aut-id=\"itemPrice\"]').text\n",
        "        except:\n",
        "            price = ''\n",
        "        try:\n",
        "            location = listing.find_element(By.XPATH, './/span[@data-aut-id=\"item-location\"]').text\n",
        "        except:\n",
        "            location = ''\n",
        "        try:\n",
        "            link = listing.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "        except:\n",
        "            link = ''\n",
        "        data.append([title, price, location, link])\n",
        "\n",
        "    output_file = \"olx_car_covers.csv\"\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Title', 'Price', 'Location', 'Link'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"ðŸŽ‰ Scraping complete! Data saved to: {os.path.abspath(output_file)}\")\n",
        "    files.download(output_file)\n",
        "\n",
        "scrape_olx_car_covers()\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "r7o7naAMiguH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}